
# https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/

import torch  # Docker implemented
import torchvision  # Docker implemented
from torchvision import transforms

import cv2

# Custom functions
from utils.custom_functions import detect, draw_roi, load_roi

from tracker.tracking_function import tracking, load_deepsort


"""
To run
(Host terminal)
xhost +
docker run --gpus all --rm -it -e DISPLAY=$DISPLAY -v  $PWD:/workspace -v /tmp/.X11-unix:/tmp/.X11-unix:rw --device="/dev/video0:/dev/video0"  yolov7-deep_sort_general:latest

"""


def detect_and_track(video_path=0,
    show_img=True, inv_h_frame=False, hold_img=False,
    save_vid=False, save_loc="result",
    model_path='yolov7.pt', class_ids=[], img_sz=640, color=(0, 255, 0),conf_thres=0.25, iou_thres=0.65,
    roi=[0, 0, 1, 1], roi_color=(255, 255, 255),
    deep_sort_model="osnet_x1_0", ds_max_dist=0.1, ds_max_iou_distance=0.7, ds_max_age=30, ds_n_init=3, ds_nn_budget=100, ds_color=(0, 0, 255)):
    """
    WHAT IT DOES:
        1. Load model of detection.
        2. Load model of tracking.
        3. Load video capture.
        4. Select the ROI.
        5. Detection.
        6. Tracking.
        7. Counting.
        8. Plots.
        9. Close capture.
        10. Return results.

    WARNINGS:
        - This function only works with CUDA enabled.
        - If you want to do your custom detection you can train and obtain the weights with the repository of YOLOv7x and then replace the .pt file.
            * https://github.com/WongKinYiu/yolov7
            * https://www.youtube.com/watch?v=wMMu_ReIaHk&list=LL&index=1


    INPUTS:
    > SOURCE PARAMETERS:
        video_path = 0 -> Path of the video or image, can it be a rstp, http, local video or webcamera.

    > OUT PARAMETERS:
        show_img = True -> To show every result in the screen.
        inv_h_frame = False -> To correct the inversion of the frame of the webcam.
        hold_img = False -> To stop the video every frame, reproducing it whith spacebar.
        color = (0, 255, 0) -> Color of the letters and bounding boxes generated by the detection module.

    > ROI PARAMETERS:
        roi = [0, 0, 1, 1] -> Sample roi.
        roi_color = (255, 255, 255) -> Color of the roi.

    > SAVE PARAMETERS:
        save_vid = False -> To save the results.
        save_loc = "result" -> Path of the results. IE: ./results.

    > DETECTION PARAMETERS (YOLOv7x):
        model_path = 'yolov7.pt' -> Path of the .pt YOLOv7x model.
        class_ids = [] -> List of class ID's that we want to detect. Empty list means 'every class'.
        img_sz = 640 -> Image size needed by the model.
        conf_thres = 0.25 -> Confidence threshold of the detection. There is a detection if the confidence is greather than conf_thres.
        iou_thres = 0.65 -> Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.

    > DEEP SORT MODEL PARAMETERS:
        deep_sort_model = "osnet_x1_0" -> Model of deep sort used. You can leave this empty and the program can recomend it one.
        ds_max_dist = 0.1 -> The matching threshold. Samples with larger distance are considered an invalid match.
        ds_max_iou_distance = 0.7 -> Gating threshold. Associations with cost larger than this value are disregarded.
        ds_max_age=30, ds_n_init = 3 -> Maximum number of missed misses before a track is deleted.
        ds_nn_budget = 100 -> Number of frames that a track remains in initialization phase.
        ds_color = (0, 0, 255) -> Maximum size of the appearance descriptors gallery.


    OUTPUTS:

        classes_after_ds = {class_id_1: number of objects of class_id_1,...,class_id_n: number of objects of class_id_n} -> Returns the number of objects detected of each class.
        names_detected = [[class_id_1, name_1],...,[class_id_n,name_n]] -> Returns the name of each class detected.
        roi = [(roi_xmin, roi_ymin), (roi_xmax, roi_ymax)] -> Returns the ROI selected.
        avg_fps = float -> Returns the AVG FPS of the detection model plus the tracking model.
        orig_w = int -> Original width of the frame.
        orig_h = int -> Original heigth of the frame.
        orig_fps = int -> Original FPS of the source.
        stopped = bool -> Returns True if the program has stopped manually and False if the video has ended.
    """

    # Graphic card setup
    if not torch.cuda.is_available():
        raise TypeError(
            'Error while trying to use Graphic Card. Please check that it is available.')

    # This functions only accept ONE graphic card located at 0, you can change the number after ':' to select other graphcard.
    # You can list the graphcards with the command 'nvidia-smi'.
    device = torch.device("cuda:0")

    # Load all characteristics of YOLOv7x model
    weigths = torch.load(model_path)

    # Send model characteristics to the graphic card
    model = weigths['model']
    model = model.half().to(device)
    _ = model.eval()

    # Get model class names
    names = model.module.names if hasattr(model, 'module') else model.names

    # Load deepsort model
    deepsort = load_deepsort(deep_sort_model=deep_sort_model, max_dist=ds_max_dist, max_iou_distance=ds_max_iou_distance,
                             max_age=ds_max_age, n_init=ds_n_init, nn_budget=ds_nn_budget)

    # Capturing the frames and excepting opening errors
    cap = cv2.VideoCapture(video_path)

    if (cap.isOpened() == False):
        raise TypeError(
            'Error while trying to read video. Please check path again')

    # Get the properties of the original capture
    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    orig_fps = cap.get(cv2.CAP_PROP_FPS) % 100

    # To save the video
    if save_vid:
        result = cv2.VideoWriter(save_loc+'.avi',
                                 cv2.VideoWriter_fourcc(*'MJPG'),
                                 10, (orig_w, orig_h))

    # Start the FPS counting
    frame_count = 0  # To count total frames.
    total_fps = 0  # To get the final frames per second.

    # Load ROI
    if show_img:
        roi = load_roi(cap, roi_color, inv_h_frame)

    else:
        if (roi[0] < 0):
            raise TypeError(
                'Error while trying to load ROI. xmin must be grater than 0')

        if (roi[1] < 0):
            raise TypeError(
                'Error while trying to load ROI. ymin must be grater than 0')

        if (roi[2] > orig_w):
            raise TypeError(
                f'Error while trying to load ROI. xmax must be smaller than {orig_w})')

        if (roi[3] > orig_h):
            raise TypeError(
                f'Error while trying to load ROI. ymax must be smaller than {orig_h})')

    # Count
    counted = []
    classes_after_ds = {}

    # While cicle of the detection
    while (cap.isOpened):

        # get the frames
        ret, frame = cap.read()

        # To show image correctly (IE: web camera)
        if inv_h_frame:
            frame = cv2.flip(frame, 1)

        # if the video has not finished yet
        if ret:

            try:
                # Detection (YOLOv7)
                coords, classes_detected, exec_time_yolo = detect(
                    frame, model, device, names, show_img=show_img, color=color, img_sz=img_sz, class_ids=class_ids, conf_thres=conf_thres, iou_thres=iou_thres)
            except:
                raise TypeError("Error while running the detection model.")

            if coords != []:

                try:
                    # Tracking (DEEP SORT osnet_x1_0)
                    output_ds, exec_time_sort = tracking(
                        coords, classes_detected, deepsort, names, frame, ds_color=ds_color, show_img=show_img)
                except:
                    raise TypeError("Error while running the tracking model.")

                # Count
                for detection in output_ds:

                    # Get variables
                    ds_cpoint = detection[0]
                    ds_id = detection[1]
                    ds_class = detection[2]

                    # To check if the ds_cpoint is into the roi
                    is_into_roi = (roi[0][0] < ds_cpoint[0] < roi[1][0]) and (
                        roi[0][1] < ds_cpoint[1] < roi[1][1])

                    # If is into the roi
                    if is_into_roi:

                        # fill the empty vector
                        if len(counted) == 0:
                            counted.append([ds_id, ds_class])

                        # get the classes detected
                            classes_after_ds = dict.fromkeys(
                                [elem[1] for elem in counted], 0)

                            # count per class
                            for elem in counted:
                                classes_after_ds[elem[1]] += 1

                        else:
                            # if the id is not in the list
                            if (ds_id not in [elem[0] for elem in counted]):
                                # count object
                                counted.append([ds_id, ds_class])

                                # get the classes detected
                                classes_after_ds = dict.fromkeys(
                                    [elem[1] for elem in counted], 0)

                                # count per class
                                for elem in counted:
                                    classes_after_ds[elem[1]] += 1
            else:
                exec_time_sort = 0

            # Calculate fps (Aproximate: 25FPS GEFORCE 1060 Max-Q Design)
            fps = 1 / (exec_time_yolo + exec_time_sort)

            total_fps += fps
            frame_count += 1

            # Show the processed frame
            if show_img:

                # draw ROI
                draw_roi(roi, roi_color, frame)

                # draw fps
                cv2.putText(frame, f"{fps:.3f} FPS (YOLO + SORT)", (15, 30), cv2.FONT_HERSHEY_SIMPLEX,
                            0.5, color, 1)

                # draw counter
                counter_text = [[key, names[key], classes_after_ds[key]]
                                for key in classes_after_ds.keys()]
                cv2.putText(frame, f"COUNTER = {counter_text}", (15, 50), cv2.FONT_HERSHEY_SIMPLEX,
                            0.5, color, 1)

                # show the frame
                cv2.imshow('PROCESSED FRAME', frame)

                # wait q to exit
                if hold_img:
                    if cv2.waitKey(0) & 0xFF == ord('q'):
                        stopped = True
                        break
                else:
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        stopped = True
                        break

        else:
            stopped = False
            break

        if save_vid:
            # to save
            result.write(frame)

    # Close the videocapture
    cap.release()

    # To save the video
    if save_vid:
        result.release()

    # Print aditional results
    avg_fps = 0
    if frame_count > 0:
        avg_fps = total_fps / frame_count

    # Close all windows
    if show_img:
        cv2.destroyAllWindows()

    return classes_after_ds, [[key, names[key]] for key in classes_after_ds.keys()], roi, round(avg_fps,2), orig_w, orig_h, orig_fps, stopped


# Test
print(detect_and_track(video_path=0,
    show_img=True, inv_h_frame=True, hold_img=False,
    save_vid=False, save_loc="results/result_3",
    model_path='pretrained_weights/yolov7.pt', class_ids=[], img_sz=640, color=(0, 255, 0),conf_thres=0.4, iou_thres=0.65,
    roi=[0, 0, 1, 1], roi_color=(255, 255, 255),
    deep_sort_model="osnet_x1_0", ds_max_dist=0.1, ds_max_iou_distance=0.7, ds_max_age=30, ds_n_init=3, ds_nn_budget=100, ds_color=(0, 0, 255)))
